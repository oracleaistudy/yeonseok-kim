{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1372358",
   "metadata": {},
   "source": [
    "### 1. 트랜스포머 가계도  \n",
    ": 디코더는 타임스텝마다 하나의 토큰을 생성하는 역할  \n",
    ": 인코더를 활용하는 또 다른 작업으로는 개체명 인식이 존재  \n",
    ": `STS`에서도 두 텍스트의 유사도를 측정하기 위해 인코더를 사용\n",
    ": 디코더 기반 모델이 인기 높인 이유 ===> 텍스트 생성 능력이 뛰어나기 때문 (but, 무에서 유를 창조 불가)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cbe7be",
   "metadata": {},
   "source": [
    "### 2. 전이 학습  \n",
    ": 이미 훈련된 모델을 새로운 작업에 맞춰 재사용하거나 약간 조정하여 사용하는 방법\n",
    ": `베이스 모델` = 합성곱 신경망의 마지막 부분에 놓인 한 개 이상의 밀집층을 버리고 입력부터 마지막 합성곱 층까지만 재사용하는 것  \n",
    ": 최근 트랜스포머 기반 언어 모델이 인기를 끄느 가장 큰 이유는 전이 학습이 가능하기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18ab97c",
   "metadata": {},
   "source": [
    "### 3. BART 모델 소개  \n",
    ": `베이스`와 `라지` 모델 두 가지 버전이 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b56cae",
   "metadata": {},
   "source": [
    "#### 3.1 BART 베이스 모델  \n",
    ": 인코더와 디코더 블록을 각각 6개씩 사용  \n",
    ": `위치 임베딩` = 임베딩 층도 처음에는 랜덤하게 초기화되며 훈련을 통해 최적의 값을 학습하는 것  \n",
    ": 디코더의 마지막 출력은 토큰에 대한 은닉 벡터 ==> 임베딩 벡터라"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8926905a",
   "metadata": {},
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a191790",
   "metadata": {},
   "source": [
    "#### 3.2 BART 인코더와 디코더  \n",
    ": 인코더와 디코더에 드롭아웃 비율이 0.1인 드롭아웃 층을 사용  \n",
    ": 드롭아웃 층 ==> 잔차 연결 ==> 층 정규화 ==> 피드포워드 네트워크(`젤루` 활성화 함수 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6cf28b",
   "metadata": {},
   "source": [
    "### 4. 허깅페이스로 KoBART 모델 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "badc160c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 08:33:04.404998: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-10 08:33:04.809465: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-10 08:33:06.701430: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(task='summarization', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ccb41d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(task='summarization', model='sshleifer/distilbart-cnn-12-6', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ac661b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \" Vincent Willem van Gogh was a Dutch Post-Impressionist painter . His oeuvre includes landscapes, still lifes, portraits and self-portraits . Van Gogh's work was beginning to gain critical attention before he died from a self-inflicted gunshot at age 37 .\"}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 딕셔너리 안에서 'summary_text'키에 매핑하여 요약된 문자열 보기\n",
    "sample_text = \"\"\"Vincent Willem van Gogh was a Dutch Post-Impressionist painter who is among the most famous and influential figures in the history of Western art. In just over a decade, he created approximately 2100 artworks, including around 860 oil paintings, most of them in the last two years of his life. His oeuvre includes landscapes, still lifes, portraits, and self-portraits, most of which are characterised by bold colours and dramatic brushwork that contributed to the rise of expressionism in modern art. Van Gogh's work was beginning to gain critical attention before he died from a self-inflicted gunshot at age 37. During his lifetime, only one of Van Gogh's paintings, The Red Vineyard, was sold.\"\"\"\n",
    "pipe(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3faebe73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096a3324674d4df7958b83df4340982c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f6452c16ff4ded9856749f7990eed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50db975805894ce5a84479b8042ef1dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a467d87c3a4e35ad46608a06d12bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af21477068a343688a99b3dde2477d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc544cc71dd54ba9aa494a3652a77f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/692 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# KoBART 모델 불러오기\n",
    "kobart = pipeline(task='summarization', model='EbanLee/kobart-summary-v3', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "537f7649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': '이 책은 데이터 분석의 핵심 내용을 7단계에 걸쳐 반복 학습하면서 머릿속에 기억되도록 구성했습니다. 독자 공부할 수 있는 커리큘럼을 그대로 믿고 끝까지 따라가다 보면 데이터 분석 공부가 난생 처음인 입문자도 무리 없이 책을 끝까지 마칠 수 있습니다. 현장감 넘치는 스토리를 통해 데이터를 다루는 방법을 알려 주어 몰입감 있는 학습을 할 수 있도록 구성했습니다. 저자가 질문 하나하나에 직접 답변을 달아 주는 것은 물론, 최신 기술과 정보도 얻을 수 있습니다. 혼공 학습단과 함께하면 마지막까지 포기하지 않고 완주할 수 있을 것입니다. '}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 지금 공부하는 도서의 소개 요약하기\n",
    "ko_text = \"\"\"\n",
    "하나, ‘입문자 맞춤형 7단계 구성’을 따라가며 체계적으로 반복하는 탄탄한 학습 설계!\n",
    "이 책은 데이터 분석의 핵심 내용을 7단계에 걸쳐 반복 학습하면서 자연스럽게 머릿속에 기억되도록 구성했습니다. [핵심 키워드]와 [시작하기 전에]에서 각 절의 주제에 대한 대표 개념을 워밍업하고, 이론과 실습을 거쳐 마무리에서는 [핵심 포인트]와 [확인 문제]로 한번에 복습합니다. ‘혼자 공부할 수 있는’ 커리큘럼을 그대로 믿고 끝까지 따라가다 보면 데이터 분석 공부가 난생 처음인 입문자도 무리 없이 책을 끝까지 마칠 수 있습니다!\n",
    "둘, 실제로 일어날 법한 흥미로운 스토리에 담긴 문제를 직접 해결하며 익히는 ‘진짜’ 데이터 분석!\n",
    "현장감 넘치는 스토리를 통해 데이터를 다루는 방법을 알려 주어 ‘파이썬’과 ‘데이터’가 낯설어도 몰입감 있는 학습을 할 수 있도록 구성했습니다. 이 책에서는 API와 웹 스크래핑을 통해 실제 도서관 데이터와 온라인 서점 웹사이트에서 데이터를 가져오는 등 내 주변에 있는 데이터를 직접 수집할 수 있는 방법을 가이드합니다. 또한 판다스, 넘파이, 맷플롯립 등 데이터 분석에 유용한 각종 파이썬 라이브러리를 활용해 보며 코딩 감각을 익히고, 핵심 통계 지식으로 기본기를 탄탄하게 다질 수 있습니다. 마지막에는 분석을 바탕으로 미래를 예측하는 머신러닝까지 맛볼 수 있어 데이터 분석의 처음부터 끝까지 제대로 배울 수 있습니다.\n",
    "셋, ‘혼공’의 힘을 실어줄 동영상 강의와 혼공 학습 사이트 지원!\n",
    "책으로만 학습하기엔 여전히 어려운 입문자를 위해 저자 직강 동영상도 지원합니다. 또한 학습을 하며 궁금한 사항은 언제든지 저자에게 질문할 수 있도록 학습 사이트를 제공합니다. 저자가 질문 하나하나에 직접 답변을 달아 주는 것은 물론, 관련 최신 기술과 정보도 얻을 수 있습니다. 게다가 혼자 공부하고 싶지만 정작 혼자서는 자신 없는 사람들을 위해 혼공 학습단을 운영합니다. 혼공 학습단과 함께하면 마지막까지 포기하지 않고 완주할 수 있을 것입니다.\n",
    "▶ https://hongong.hanbit.co.kr\n",
    "▶ https://github.com/rickiepark/hg-da\n",
    "넷, 언제 어디서든 가볍게 볼 수 있는 혼공 필수 [용어 노트] 제공!\n",
    "꼭 기억해야 할 핵심 개념과 용어만 따로 정리한 [용어 노트]를 제공합니다. 처음 공부하는 사람들이 프로그래밍을 어려워하는 이유는 낯선 용어 때문입니다. 그러나 어려운 것이 아니라 익숙하지 않아서 헷갈리는 것이므로, 용어나 개념이 잘 생각나지 않을 때는 언제든 부담 없이 [용어 노트]를 펼쳐 보세요. 제시된 용어 외에도 새로운 용어를 추가하면서 자신만의 용어 노트를 완성해가는 과정도 또 다른 재미가 될 것입니다.\n",
    "\"\"\"\n",
    "\n",
    "kobart(ko_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac2fc35",
   "metadata": {},
   "source": [
    "- 이 모델은 기본적으로 최대 300자까지 요약을 만들기 때문에 이전 예제보다 요약 결과가 조금 더 긴 것을 확인 가능  \n",
    "- `빔 서치`라는 방법을 사용해 텍스트를 생성 ==> 실행할 때마다 결과가 다를 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5154c3",
   "metadata": {},
   "source": [
    "### 5. 텍스트 토큰화  \n",
    ": `토큰화`는 텍스트를 `토큰`이라는 단위로 분할하는 과정  \n",
    ": 이런 토큰화는 LLM이 수행하지 않고 이미 텍스트가 토큰으로 분할돠고 각 토큰에 정수 아이디가 할당된 후 이 정수 리스트가 전달된다고 가정  \n",
    ": 토큰화를 수행하는 모델은 `토크나이저`부르며 모델과 별도로 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d54ee3",
   "metadata": {},
   "source": [
    "#### 5.1 BPE  \n",
    ": 각 단어를 문자 단위로 분해하여 어휘 사전에 추가한 후, 가장 많이 등장하는 순서대로 문자 쌍을 찾아 병합  \n",
    ": `바이트 수준의 BPE`는 텍스트를 바이트 스트림으로 인식하고 자주 등장하는 바이트 쌍을 어휘사전에 추가하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f23e321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "print(kobart.tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24688a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kobart.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fcdc38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = kobart.tokenizer.vocab\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac1207c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('픲', 13564),\n",
       " ('▁이외에도', 23825),\n",
       " ('짹', 12379),\n",
       " ('이', 12034),\n",
       " ('▁보고서는', 27424),\n",
       " ('▁음', 14359),\n",
       " ('▁‘무', 25891),\n",
       " ('공헌', 21752),\n",
       " ('猴', 5631),\n",
       " ('▁놓여', 22503)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# items() 메서드의 출력을 리스트로 바꾼 다음 맨 처음 토큰 몇 개를 확인\n",
    "list(vocab.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10178c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁혼자', '▁만들', '면서', '▁공부', '하는', '▁', '딥', '러', '닝']\n"
     ]
    }
   ],
   "source": [
    "tokens = kobart.tokenizer.tokenize('혼자 만들면서 공부하는 딥러닝')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c93993",
   "metadata": {},
   "source": [
    "8개의 토큰으로 나눠짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "090f489b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16814, 14397, 14125, 16962, 14049, 1700, 10021, 10277, 9747]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kobart.tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c18d292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 16814, 14397, 14125, 16962, 14049, 1700, 10021, 10277, 9747, 1]\n"
     ]
    }
   ],
   "source": [
    "token_ids = kobart.tokenizer.encode('혼자 만들면서 공부하는 딥러닝')\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833f4e4f",
   "metadata": {},
   "source": [
    "- 맨 앞과 뒤에 토큰 아이디 0과 1은 문자열의 시작과 끝을 나타냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee36c1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁혼자', '▁만들', '면서', '▁공부', '하는', '▁', '딥', '러', '닝', '</s>']\n"
     ]
    }
   ],
   "source": [
    "tokens = kobart.tokenizer.convert_ids_to_tokens(token_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64bfdc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> 혼자 만들면서 공부하는 딥러닝</s>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 리스트를 원래 문자열로 복원 ==> decode() 메서드 사용\n",
    "kobart.tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b9b240",
   "metadata": {},
   "source": [
    "#### 5.2 워드피스  \n",
    ": 워드피스는 부분단어를 구성하는 개별 토큰의 빈도도 고려  \n",
    ": `BERT` = 워드피스를 사용하는 대표적인 LLM으로 인코더 기반 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c64a11c",
   "metadata": {},
   "source": [
    "#### 5.3 유니그램  \n",
    ": 초기에 매우 큰 어휘 사전을 만든 다음 사전에 지정한 어휘 사전 크기에 도달할 때까지 점진적으로 토큰을 제거  \n",
    ": 전체 손실을 가장 적게 증가시키는 토큰을 하나씩 삭제  \n",
    ": 손실은 음의 로그 확률이 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac02ec2",
   "metadata": {},
   "source": [
    "#### 센텐스피스  \n",
    ": `사전 토큰화` = 모든 토큰화 방법은 먼저 텍스트를 공백 등을 기준으로 단어로 나눔  \n",
    ": 센텐스피스는 원시 입력 텍스트를 그대로 사용하며 공백을 문자의 하나로 간줌"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
